{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L’Allocation Latente de Dirichlet : un aperçu fonctionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Présentation du modèle LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:justify'>\n",
    "    Modèle phare du **topic modeling**, l’Allocation Latente de Dirichlet ou LDA est une technique de choix pour l’analyse de larges collections de texte. Elle répond à des questions telles que : « de quoi parle un document ? », « quels documents parlent de la même chose ? » ou encore « quels documents dois-je lire si je m’intéresse à tel sujet ? ». A la base de ce modèle se trouvent deux hypothèses fortes, selon lesquelles *1/ dans un (vaste) corpus textuel, les thématiques dont parlent les documents sont indépendantes entre elles* ; *2/ les mots constitutifs de ce document sont échangeables entre eux* ; on parle d’approche « sac de mots », ou « bag of words ». Ainsi, les phrases : « Grosminet a mangé Titi » et « Titi a mangé Grosminet » sont parfaitement équivalentes du point de vue de ce modèle. Bien que cette équivalencepuisse choquer tant la sémantique est différente pour le lecteur humain, ces hypothèses sont pourtant parmi celles à l’origine du succès et surtout des performances de la LDA.\n",
    "</p>\n",
    "\n",
    "<p style='text-align:justify'>\n",
    "    Lorsque l’on parle de de texte, trois niveaux de granularité sont ici à considérer, de la plus fine à la plus grossière : *1/ le mot*, qui est l’unité de base d’un vocabulaire ; *2/ le document*, soit une séquence de mots ; *3/ le corpus*, c’est-à-dire une collection de documents.\n",
    "</p>\n",
    "\n",
    "<figure>\n",
    "    <img src='img/text.svg'/>\n",
    "    <figcaption style='text-align:center'>*Les trois \"composantes\" du texte*</figcaption>\n",
    "</figure>\n",
    "\n",
    "<p style='text-align:justify'>\n",
    "    Chaque document appartient à une voire plusieurs *thématiques* (ou *topics*) connues d’avance : on considère que ce sont des **mélanges** de thématiques, dans des proportions diverses. Il s’agit d’ailleurs de l’un des principaux apports de la LDA par rapport à d’autres modèles comme l’*unigram* et ses dérivés, qui rattachent un document à une et une seule thématique. Les thématiques sont à leur tour considérées comme des mélanges de termes qui apparaissent dans les documents . Concrètement, du point de vue de l’algorithme, un corpus est un vecteur de documents et un document sera considéré comme un vecteur dont les coordonnées seront le nombre d’occurrences de chaque mot du vocabulaire.\n",
    "</p>\n",
    "<p style='text-align:justify'>\n",
    "    Le modèle LDA a trois propriétés essentielles : *1/* c’est un modèle *probabiliste* ; *2/* c’est un modèle d’*apprentissage non supervisé* ; *3/* c’est un modèle *génératif*. C’est un modèle probabiliste et d’apprentissage non supervisé parce qu’on va chercher à trouver une correspondance entre ce que l’on observe, c’est-à-dire les mots d’un corpus textuel donné en entrée, et un espace probabilisé, où se trouvent les proportions exactes dans lesquelles les thématiques sont abordées<sup>1</sup>, sans pour autant qu’on les connaisse d’avance<sup>2</sup>. C’est un modèle génératif, car on va chercher à apprendre la distribution selon laquelle les documents ont été générés, à estimer des paramètres cachés, comme si l’on cherchait à en générer de nouveaux à partir de la nouvelle distribution<sup>3</sup>. Cette dernière propriété est essentielle, car elle va permettre *1/* de limiter le nombre de paramètres entrant en compte dans le modèle et donc de prévenir des effets de surapprentissage, notamment dans le cas où l’on chercherait à classifier post-exécution de la LDA *2/* de réduire la dimensionnalité du corpus. Il s’agit ici d’une plus-value du modèle par rapport à la *(probabilistic) Latent Semantic Indexing* par exemple, dont le nombre de paramètres à estimer augmente linéairement par rapport à la taille du corpus d’entraînement.\n",
    "</p>\n",
    "<p style='font-size:0.9em;text-align:justify'>\n",
    "    [1] On pourrait également dire que l'on admet que les données observées sont des réalisations de variables aléatoires<br/>\n",
    "    [2] Les thématiques en elles-mêmes ainsi que leur nombre sont en revanche connues.<br/>\n",
    "    [3] On parle ici d’inférence *a posteriori*.<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='img/lda.svg'/>\n",
    "    <figcaption style='text-align:center'>*Modèle graphique de l'Allocation Latente de Dirichlet*</figcaption>\n",
    "    <figcaption style='text-align:center;font-size:0.9em'>*(en gris : les mots, seuls éléments observés)*</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:justify'>Selon la LDA, la mécanique de génération d'un document est la suivante :</p>\n",
    "\n",
    "* Pour un corpus C = (d<sub>1</sub>, …, d<sub>M</sub>) de M documents :\n",
    "    * Pour chaque document d<sub>i</sub> = (w<sub>1</sub>, …, w<sub>Ni</sub>) de N<sub>i</sub> mots :\n",
    "      * On choisit N<sub>i</sub>~Poisson(ξ)\n",
    "      * On choisit θ~Dir(α)\n",
    "      * Pour chacun des N<sub>i</sub> mots de w<sub>n</sub> de d<sub>i</sub> :\n",
    "        * On choisit une thématique z<sub>n</sub>~Multinomiale(θ)\n",
    "        * On choisit un mot w<sub>n</sub> de p(w<sub>n</sub> | z<sub>n</sub>, β)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:justify'>Cinq paramètres, dont trois sont des hyperparamètres, régissent ce modèle :</p>\n",
    "\n",
    "* ξ : cet hyperparamètre va servir à choisir le nombre de mots N<sub>i</sub> à générer pour chaque document. La variable N suit une loi de Poisson, qui est adaptée pour la modélisation d’événements rares, mais on peut lui substituer d’autres distributions en fonction des besoins étant donné que cette variable n’interagit pas avec celles que l’on veut estimer.\n",
    "\n",
    "* β : il s’agit d’une matrice K x V<sup>1</sup>, c’est-à-dire nombre de topics x vocabulaire, associée au corpus.\n",
    "\n",
    "* α : il s’agit du vecteur de dimension K<sup>3</sup> correspondant au paramètre de la loi de Dirichlet<sup>2</sup> à la base du modèle. Concrètement, il s’agit des proportions moyennes des thématiques que l’on peut observer sur les données d’entraînement.\n",
    "\n",
    "* θ : il s’agit du premier paramètre latent du modèle. Il représente la proportion exacte des thématiques dans chaque document. Il suit la loi de Dirichlet de paramètre α.\n",
    "\n",
    "* z : il s’agit du deuxième paramètre latent du modèle. Il s’agit des thématiques associées à chaque mot de chaque document.\n",
    "\n",
    "<p style='font-size:0.9em;text-align:justify'>\n",
    "    [1] V représente le vocabulaire<br/>\n",
    "    [2] Rappel : K correspond au nombre de thématiques que l'on choisit de considérer.<br/>\n",
    "    [3] Loi de probabilité utile pour étudier des proportions et surtout leur variation.<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='img/lda2.png'/>\n",
    "    <figcaption style='text-align:center'>*Illustration de la génération d'un document*</figcaption>\n",
    "</figure>\n",
    "\n",
    "<p style='text-align:justify'>\n",
    "Quelques éclairages sur cette mécanique semblent nécessaires, que nous apporterons à travers un exemple. Imaginons que nous disposions d'un corpus de documents caractérisés par trois thématiques A, B et C, chacune d'entre elles étant décrites par des mots différents. Nous voulons générer un nouveau document qui parlerait à 50% de A, 30% de B et 20% de C. On choisit un nombre de mots pour ce corpus puis une thématique et des mots qui lui sont associés en se basant sur les proportions de notre choix. D'après les proportions que nous avons choisies, pour un document de 1000 mots, il y en aurait 500 provenant du topic A, 300 relevant du topic B et 200 correspondant au topic C.\n",
    "</p>\n",
    "<p style='text-align:justify'>\n",
    "    Toute la question est maintenant celle du processus d'apprentissage de l'algorithme. Le problème du processus génératif tel qu'il est défini est qu'il est en pratique difficile voire impossible à calculer du fait des liens entre θ et β. Il va donc falloir simplifier le modèle, c'est-à-dire redéfinir une famille de distribution sur les variables latentes pour pouvoir utliser des techniques d'approximation des paramètres, comme l'algorithme *Expectation-Maximization* (*EM*) et notamment sa variante variationnelle. En particulier, on va chercher à maximiser la log-vraisemblance suivante : <br /><br />\n",
    "    <div style='text-align:center'>$\\ell(\\alpha,\\beta)=\\sum_{d=1}^{M}\\log p({w_{d}}\\mid \\alpha,\\beta)$</div><br /> <br />\n",
    "Grâce à cette méthode, nous pourrons *in fine* savoir quels mots sont les plus rattachés à quel topic et surtout quels sont les plus représentatifs de ce topic.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Démonstration des packages <code>topicmodels</code> et <code>topicmodels</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:justify'>\n",
    "    Le package <code>topicmodels</code> fournit une interface avec le code C de la LDA de Blei et al. L'avantage de l'utilisation de ce package est double : *1/* nous avons accès à une implémentation fiable de la LDA, avec une méthode d'estimation connue (*méthode de Gibbs*); *2/* ce package est compatible avec le <code>tidyverse</code>, qui est un ensemble d'outils permettant de manipuler, nettoyer et traiter des données<sup>1</sup>. La partie du <code>tidyverse</code> qui nous intéresse le plus est <code>tidytext</code>. Les instructions qui suivent ont vocation à installer le package et ses dépendances ainsi qu'à en fournir une démonstration<sup>2</sup>.\n",
    "</p>\n",
    "\n",
    "<p style='font-size:0.9em;text-align:justify'>\n",
    "    [1] [Plus d'informations sur le tidyverse](https://juba.github.io/tidyverse/index.html)<br/>\n",
    "    [2] Source : [Text Mining with R - A tidy approach](https://www.tidytextmining.com/)<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"topicmodels\")\n",
    "library(topicmodels)\n",
    "data(\"AssociatedPress\")\n",
    "ap_lda <- LDA(AssociatedPress, k=2, control=list(seed=1234))\n",
    "print(ap_lda)\n",
    "\n",
    "library(tidytext)\n",
    "\n",
    "ap_topics <- tidy(ap_lda, matrix = \"beta\")\n",
    "ap_topics\n",
    "\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "ap_top_terms <- ap_topics %>%\n",
    "  group_by(topic) %>%\n",
    "  top_n(10, beta) %>%\n",
    "  ungroup() %>%\n",
    "  arrange(topic, -beta)\n",
    "\n",
    "ap_top_terms %>%\n",
    "  mutate(term = reorder(term, beta)) %>%\n",
    "  ggplot(aes(term, beta, fill = factor(topic))) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ topic, scales = \"free\") +\n",
    "  coord_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
